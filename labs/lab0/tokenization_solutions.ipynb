{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 1: Tokenization Exercises - Solutions\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  <font color='green'>Setup 1</font>: Load Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='blue'>Task 1 solution</font>\n",
    "\n",
    "To tokenise the following text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"'Curiouser and curiouser!' cried Alice (she was so much surprised, that for the moment she quite\n",
    "forgot how to speak good English); 'now I'm opening out like the largest telescope that ever was! Good-bye,\n",
    "feet!' (for when she looked down at her feet, they seemed to be almost out of sight, they were getting so far\n",
    "off). 'Oh, my poor little feet, I wonder who will put on your shoes and stockings for you now, dears? I'm sure I\n",
    "shan't be able! I shall be a great deal too far off to trouble myself about you: you must manage the best\n",
    "way you can; —but I must be kind to them,' thought Alice, 'or perhaps they won't walk the way I want to go!\n",
    "Let me see: I'll give them a new pair of boots every Christmas.'\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to expand the list of tokens a bit to account for:\n",
    "- additional characters like ! ( ) , ; — - (notice the difference in the last two dashes)\n",
    "- separation of `n't` from the rest of the word (question: why?)\n",
    "- `'ll` `'m` (question: why?)\n",
    "\n",
    "Should you need additional help on regular expressions, check https://regex101.com/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'\n",
      "Curiouser\n",
      "and\n",
      "curiouser\n",
      "!\n",
      "'\n",
      "cried\n",
      "Alice\n",
      "(\n",
      "she\n",
      "was\n",
      "so\n",
      "much\n",
      "surprised\n",
      ",\n",
      "that\n",
      "for\n",
      "the\n",
      "moment\n",
      "she\n",
      "quite\n",
      "forgot\n",
      "how\n",
      "to\n",
      "speak\n",
      "good\n",
      "English\n",
      ")\n",
      ";\n",
      "'\n",
      "now\n",
      "I\n",
      "'m\n",
      "opening\n",
      "out\n",
      "like\n",
      "the\n",
      "largest\n",
      "telescope\n",
      "that\n",
      "ever\n",
      "was\n",
      "!\n",
      "Good\n",
      "-\n",
      "bye\n",
      ",\n",
      "feet\n",
      "!\n",
      "'\n",
      "(\n",
      "for\n",
      "when\n",
      "she\n",
      "looked\n",
      "down\n",
      "at\n",
      "her\n",
      "feet\n",
      ",\n",
      "they\n",
      "seemed\n",
      "to\n",
      "be\n",
      "almost\n",
      "out\n",
      "of\n",
      "sight\n",
      ",\n",
      "they\n",
      "were\n",
      "getting\n",
      "so\n",
      "far\n",
      "off\n",
      ")\n",
      ".\n",
      "'\n",
      "Oh\n",
      ",\n",
      "my\n",
      "poor\n",
      "little\n",
      "feet\n",
      ",\n",
      "I\n",
      "wonder\n",
      "who\n",
      "will\n",
      "put\n",
      "on\n",
      "your\n",
      "shoes\n",
      "and\n",
      "stockings\n",
      "for\n",
      "you\n",
      "now\n",
      ",\n",
      "dears\n",
      "?\n",
      "I\n",
      "'m\n",
      "sure\n",
      "I\n",
      "sha\n",
      "n't\n",
      "be\n",
      "able\n",
      "!\n",
      "I\n",
      "shall\n",
      "be\n",
      "a\n",
      "great\n",
      "deal\n",
      "too\n",
      "far\n",
      "off\n",
      "to\n",
      "trouble\n",
      "myself\n",
      "about\n",
      "you\n",
      ":\n",
      "you\n",
      "must\n",
      "manage\n",
      "the\n",
      "best\n",
      "way\n",
      "you\n",
      "can\n",
      ";\n",
      "—\n",
      "but\n",
      "I\n",
      "must\n",
      "be\n",
      "kind\n",
      "to\n",
      "them\n",
      ",\n",
      "'\n",
      "thought\n",
      "Alice\n",
      ",\n",
      "'\n",
      "or\n",
      "perhaps\n",
      "they\n",
      "wo\n",
      "n't\n",
      "walk\n",
      "the\n",
      "way\n",
      "I\n",
      "want\n",
      "to\n",
      "go\n",
      "!\n",
      "Let\n",
      "me\n",
      "see\n",
      ":\n",
      "I\n",
      "'ll\n",
      "give\n",
      "them\n",
      "a\n",
      "new\n",
      "pair\n",
      "of\n",
      "boots\n",
      "every\n",
      "Christmas\n",
      ".\n",
      "'\n"
     ]
    }
   ],
   "source": [
    "token = re.compile(\"[\\w]+(?=n't)|n't|\\'m|\\'ll|[\\w]+|[.?!;,\\-\\(\\)—\\:']\")\n",
    "tokens = token.findall(text)\n",
    "for t in tokens:\n",
    "    print(t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='blue'>Task 2 solution</font>\n",
    "\n",
    "The following token from [UCLMR tweet](https://twitter.com/IAugenstein/status/766628888843812864):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet = \"#emnlp2016 paper on numerical grounding for error correction http://arxiv.org/abs/1608.04147  @geospith @riedelcastro #NLProc\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "can be tokenised in various ways, but the main objective here is to 'catch' hashtags, mentions and URLs. Hashtags and mentions can be extracted easily by just simply adding `#` and `@` to the regular expression part which catches alphanumeric sequences.\n",
    "\n",
    "However, catching URLs is a bit more tricky, and the minimum working example for this case would be the coded example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['#emnlp2016', 'paper', 'on', 'numerical', 'grounding', 'for', 'error', 'correction', 'http://arxiv.org/abs/1608.04147', '@geospith', '@riedelcastro', '#NLProc']\n"
     ]
    }
   ],
   "source": [
    "# hashtags and user mentions should be included, as well as the hyperlinks - there are more elaborate URL regular expressions, but this one will do for now\n",
    "token = re.compile('http://[a-zA-Z0-9./]+|[@#\\w]+')\n",
    "tokens = token.findall(tweet)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, bear in mind that this is far from a correct solution for capturing URLs, and that many valid URLs would not be correctly caught with this expression (think https, querystrings, etc.)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
